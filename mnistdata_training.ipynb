{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils and modules\n",
    "import StochasticGradientAscentUtils as sga\n",
    "import derivatives as der\n",
    "import activationFunctions as activate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os,glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and store our data from MNISTDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store our file names for better usage\n",
    "def make_filePaths_to_arr():\n",
    "    print(\"Trying to find all files in mnistdata directory!\")\n",
    "    files = glob.glob('*/*.txt')\n",
    "    return files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to find all files in mnistdata directory!\n"
     ]
    }
   ],
   "source": [
    "# split array into test_paths and train_paths\n",
    "files = make_filePaths_to_arr()\n",
    "test_file_paths = [files[i] for i in range(10)]\n",
    "train_file_paths = [files[i] for i in range(10,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnistdata\\\\test0.txt',\n",
       " 'mnistdata\\\\test1.txt',\n",
       " 'mnistdata\\\\test2.txt',\n",
       " 'mnistdata\\\\test3.txt',\n",
       " 'mnistdata\\\\test4.txt',\n",
       " 'mnistdata\\\\test5.txt',\n",
       " 'mnistdata\\\\test6.txt',\n",
       " 'mnistdata\\\\test7.txt',\n",
       " 'mnistdata\\\\test8.txt',\n",
       " 'mnistdata\\\\test9.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnistdata\\\\train0.txt',\n",
       " 'mnistdata\\\\train1.txt',\n",
       " 'mnistdata\\\\train2.txt',\n",
       " 'mnistdata\\\\train3.txt',\n",
       " 'mnistdata\\\\train4.txt',\n",
       " 'mnistdata\\\\train5.txt',\n",
       " 'mnistdata\\\\train6.txt',\n",
       " 'mnistdata\\\\train7.txt',\n",
       " 'mnistdata\\\\train8.txt',\n",
       " 'mnistdata\\\\train9.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which reads every test and train .txt ( image ), \n",
    "# store every row from the dataframe in an array ( RGB values for every image ) \n",
    "# and returns the 2 arrays with every single image.\n",
    "# train_images : stores in 2D np.array every train image\n",
    "# test_images : stores in 2D np.array every test image\n",
    "def load_images(paths):\n",
    "    train_images, test_images = [], []    # initialize dynamic arrays\n",
    "    for i in range(len(paths[0])):        # for every train(i).txt ( eg. train0.txt, train1.txt etc. )\n",
    "        df = pd.read_csv(paths[0][i],header=None,sep=' ')      # take every file as dataframe\n",
    "        # read every row and take every column's value ( RGB )\n",
    "        for row in range(df.shape[0]):\n",
    "            current_row = np.array(df.iloc[row:row+1,:])   # store the 2D array\n",
    "            train_images.append(current_row.flatten())     # flatten the 2D into 1D for better usage, and save it to our array.\n",
    "            \n",
    "    for i in range(len(paths[1])):           # for every train(i).txt ( eg. train0.txt, train1.txt etc. )\n",
    "        df = pd.read_csv(paths[1][i],header=None,sep=' ')      # take every file as dataframe\n",
    "         # read every row and take every column's value ( RGB )\n",
    "        for row in range(df.shape[0]):\n",
    "            current_row = np.array(df.iloc[row:row+1,:])      # store the 2D array\n",
    "            test_images.append(current_row.flatten())          # flatten the 2D into 1D for better usage, and save it to our array.\n",
    "            \n",
    "    train_images, test_images = np.array(train_images), np.array(test_images)        # convert them into np.arrays\n",
    "    \n",
    "    train_images = train_images.astype(float)/255\n",
    "    test_images = test_images.astype(float)/255\n",
    "    \n",
    "    return train_images, test_images      # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_true_values(paths):\n",
    "    train_true_values = []\n",
    "    for i in range(len(paths[0])):\n",
    "        one_hot = [1 if i==k else 0 for k in range(len(paths[0]))]\n",
    "        df = pd.read_csv(paths[0][i],header=None,sep=' ')\n",
    "        for k in range(df.shape[0]):\n",
    "            train_true_values.append(one_hot)\n",
    "    test_true_values = []\n",
    "    for i in range(len(paths[1])):\n",
    "        one_hot = [1 if i==k else 0 for k in range(len(paths[1]))]\n",
    "        df = pd.read_csv(paths[1][i],header=None,sep=' ')\n",
    "        for k in range(df.shape[0]):\n",
    "            test_true_values.append(one_hot)\n",
    "\n",
    "    y_train, y_test  = np.array(train_true_values), np.array(test_true_values)\n",
    "    return y_train, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# check if we read every .txt file correctly\n",
    "paths = [train_file_paths, test_file_paths]\n",
    "X_train, X_test = load_images(paths)\n",
    "Y_train, Y_test = store_true_values(paths)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradcheck for random hyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gradW1 , num_w1, gradW2, num_w2 = sga.gradcheck(X_train, Y_train, 0.1, 200, 100, 10, \"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of W1 is :  7.170281368651388e-09\n"
     ]
    }
   ],
   "source": [
    "print( \"Gradient of W1 is : \", np.max(np.abs(gradW1 - num_w1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of W2 is :  7.069218543875877e-09\n"
     ]
    }
   ],
   "source": [
    "print( \"Gradient of W2 is : \", np.max(np.abs(gradW2 - num_w2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing hyperParameters for mnistdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testParameters(X, T, x_test, y_test):\n",
    "    \n",
    "    X_Test = np.copy(x_test)\n",
    "    \n",
    "    add_one = np.ones((X_Test.shape[0],1))\n",
    "    X_Test = np.concatenate((add_one,X_Test), axis=1)\n",
    "    \n",
    "    lamdas = [0.1, 0.5, 0.01]\n",
    "    learning_rates = [0.01, 0.001, 0.05]\n",
    "    EPOCHS = [10, 20, 30]\n",
    "    Ms = [100, 200, 300]\n",
    "    activation_h = [\"h1\", \"h2\", \"h3\"]\n",
    "    batch_size = [200]\n",
    "    \n",
    "    results = {\"batch_size\" : [],\n",
    "              \"HiddenLayers M\" : [],\n",
    "              \"activation_h\" : [],\n",
    "              \"epochs\" : [],\n",
    "              \"learning_rate\" : [],\n",
    "              \"lamda\" : [],\n",
    "               \"error\" : [],\n",
    "              \"accuracy\" : []\n",
    "              }\n",
    "    \n",
    "    scores = []\n",
    "    for batchSize in batch_size:\n",
    "        for M in Ms:\n",
    "            for h in activation_h:\n",
    "                for epoch in EPOCHS:\n",
    "                    print(\"Current M: \", M, \" | Current h: \", h, \" | Current epoch: \", epoch)\n",
    "                    for l_rate in learning_rates:\n",
    "                        for l in lamdas:\n",
    "                            w1, w2 ,costs = sga.mini_batches_SGA(X, T,activation_h = h, hiddenLayers = M, classesK = 10, learning_rate = l_rate, lamda = l, epochs = epoch, batch_size = batchSize)\n",
    "                            pred, _, _ = sga.NeuralNetwork(X_Test, w1, w2, hiddenLayerSize = M, outLayerSize = 10, activation_h = h)\n",
    "                            acc = np.mean( np.argmax(pred,1) == np.argmax(y_test ,1) )\n",
    "                            scores.append(acc) # save score we got\n",
    "                            results['batch_size'].append(batchSize)\n",
    "                            results['HiddenLayers M'].append(M)\n",
    "                            results['activation_h'].append(h)\n",
    "                            results['epochs'].append(epoch)\n",
    "                            results['learning_rate'].append(l_rate)\n",
    "                            results['lamda'].append(l)\n",
    "                            results['error'].append(1-acc)\n",
    "                            results['accuracy'].append(acc)\n",
    "    return scores, results\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current M:  100  | Current h:  h1  | Current epoch:  10\n",
      "Current M:  100  | Current h:  h1  | Current epoch:  20\n",
      "Current M:  100  | Current h:  h1  | Current epoch:  30\n",
      "Current M:  100  | Current h:  h2  | Current epoch:  10\n",
      "Current M:  100  | Current h:  h2  | Current epoch:  20\n",
      "Current M:  100  | Current h:  h2  | Current epoch:  30\n",
      "Current M:  100  | Current h:  h3  | Current epoch:  10\n",
      "Current M:  100  | Current h:  h3  | Current epoch:  20\n",
      "Current M:  100  | Current h:  h3  | Current epoch:  30\n",
      "Current M:  200  | Current h:  h1  | Current epoch:  10\n",
      "Current M:  200  | Current h:  h1  | Current epoch:  20\n",
      "Current M:  200  | Current h:  h1  | Current epoch:  30\n",
      "Current M:  200  | Current h:  h2  | Current epoch:  10\n",
      "Current M:  200  | Current h:  h2  | Current epoch:  20\n",
      "Current M:  200  | Current h:  h2  | Current epoch:  30\n",
      "Current M:  200  | Current h:  h3  | Current epoch:  10\n",
      "Current M:  200  | Current h:  h3  | Current epoch:  20\n",
      "Current M:  200  | Current h:  h3  | Current epoch:  30\n",
      "Current M:  300  | Current h:  h1  | Current epoch:  10\n",
      "Current M:  300  | Current h:  h1  | Current epoch:  20\n",
      "Current M:  300  | Current h:  h1  | Current epoch:  30\n",
      "Current M:  300  | Current h:  h2  | Current epoch:  10\n",
      "Current M:  300  | Current h:  h2  | Current epoch:  20\n",
      "Current M:  300  | Current h:  h2  | Current epoch:  30\n",
      "Current M:  300  | Current h:  h3  | Current epoch:  10\n",
      "Current M:  300  | Current h:  h3  | Current epoch:  20\n",
      "Current M:  300  | Current h:  h3  | Current epoch:  30\n",
      "Wall time: 3h 47min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores, data = testParameters(X_train, Y_train, X_test, Y_test)\n",
    "df = pd.DataFrame(data, columns= ['batch_size', 'HiddenLayers M', 'activation_h', 'epochs', 'learning_rate', 'lamda', 'error', 'accuracy'])\n",
    "df.to_csv(\"MNISTDATA_RESULTS_TEST_PARAMETERS.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
