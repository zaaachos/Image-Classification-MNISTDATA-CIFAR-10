{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils and modules\n",
    "import StochasticGradientAscentUtils as sga\n",
    "import derivatives as der\n",
    "import activationFunctions as activate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os,glob\n",
    "from six.moves import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(labels):\n",
    "    maxval = np.max(labels)\n",
    "    return np.eye(maxval+1)[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and store our data from CIFAR-10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store our file names for better usage\n",
    "def make_filePaths_to_arr():\n",
    "    print(\"Trying to find all files in cifar-10-batches-py directory!\")\n",
    "    files = glob.glob('*/*')\n",
    "    CIFAR = [f for f in files if \"cifar\" in f and \".\" not in f]   #store only the batch_data\n",
    "    return CIFAR\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to find all files in cifar-10-batches-py directory!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cifar-10-batches-py\\\\data_batch_1',\n",
       " 'cifar-10-batches-py\\\\data_batch_2',\n",
       " 'cifar-10-batches-py\\\\data_batch_3',\n",
       " 'cifar-10-batches-py\\\\data_batch_4',\n",
       " 'cifar-10-batches-py\\\\data_batch_5',\n",
       " 'cifar-10-batches-py\\\\test_batch']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split array into test_paths and train_paths\n",
    "files = make_filePaths_to_arr()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_paths = [path for path in files[:5]]\n",
    "test_file_paths = files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data():\n",
    "    batches = []\n",
    "    for file in train_file_paths:\n",
    "        with open(file, 'rb') as f:\n",
    "            print(\"Loading: \" + file)\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            X = data['data']\n",
    "            Y = oneHotEncoding(data['labels'])\n",
    "            X = X.reshape(10000,3072)\n",
    "            Y = np.array(Y)\n",
    "            batch = [X,Y]\n",
    "            batches.append(batch)\n",
    "    with open(test_file_paths, 'rb') as f:\n",
    "            print(\"Loading: \" + test_file_paths)\n",
    "            data = pickle.load(f, encoding='latin1')\n",
    "            X_test = data['data']\n",
    "            Y_test = oneHotEncoding(data['labels'])\n",
    "            X_test = X_test.reshape(10000,3072)\n",
    "            Y_test = np.array(Y_test)\n",
    "            \n",
    "    return batches, X_test, Y_test        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatData():\n",
    "    batches, x_test, y_test = load_image_data()\n",
    "    X_train, Y_train = [], []\n",
    "    for batch in batches:\n",
    "        batchX = batch[0]\n",
    "        batchY = batch[1]\n",
    "        X_train.extend(batchX)\n",
    "        Y_train.extend(batchY)\n",
    "        \n",
    "    return np.array(X_train), np.array(Y_train), x_test, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: cifar-10-batches-py\\data_batch_1\n",
      "Loading: cifar-10-batches-py\\data_batch_2\n",
      "Loading: cifar-10-batches-py\\data_batch_3\n",
      "Loading: cifar-10-batches-py\\data_batch_4\n",
      "Loading: cifar-10-batches-py\\data_batch_5\n",
      "Loading: cifar-10-batches-py\\test_batch\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = concatData()\n",
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradcheck for random hyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gradW1 , num_w1, gradW2, num_w2 = sga.gradcheck(X_train, Y_train, 0.1, 200, 100, 10, \"h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of W1 is :  1.3028934375824974e-08\n"
     ]
    }
   ],
   "source": [
    "print( \"Gradient of W1 is : \", np.max(np.abs(gradW1 - num_w1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of W2 is :  1.0419002771655528e-08\n"
     ]
    }
   ],
   "source": [
    "print( \"Gradient of W2 is : \", np.max(np.abs(gradW2 - num_w2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing hyperParameters for CIFAR-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testParameters(X, T, x_test, y_test):\n",
    "    \n",
    "    X_Test = np.copy(x_test)\n",
    "    \n",
    "    add_one = np.ones((X_Test.shape[0],1))\n",
    "    X_Test = np.concatenate((add_one,X_Test), axis=1)\n",
    "    \n",
    "    lamdas = [0.1, 0.5]\n",
    "    learning_rates = [0.01, 0.001]\n",
    "    EPOCHS = [10, 20]\n",
    "    Ms = [100, 200, 300]\n",
    "    activation_h = [\"h1\", \"h2\", \"h3\"]\n",
    "    batch_size = [200]\n",
    "    \n",
    "    results = {\"batch_size\" : [],\n",
    "              \"HiddenLayers M\" : [],\n",
    "              \"activation_h\" : [],\n",
    "              \"epochs\" : [],\n",
    "              \"learning_rate\" : [],\n",
    "              \"lamda\" : [],\n",
    "               \"error\" : [],\n",
    "              \"accuracy\" : []\n",
    "              }\n",
    "    \n",
    "    scores = []\n",
    "    for batchSize in batch_size:\n",
    "        for M in Ms:\n",
    "            for h in activation_h:\n",
    "                for epoch in EPOCHS:\n",
    "                    print(\"Current M: \", M, \" | Current h: \", h, \" | Current epoch: \", epoch)\n",
    "                    for l_rate in learning_rates:\n",
    "                        for l in lamdas:\n",
    "                            w1, w2 ,costs = sga.mini_batches_SGA(X, T,activation_h = h, hiddenLayers = M, classesK = 10, learning_rate = l_rate, lamda = l, epochs = epoch, batch_size = batchSize)\n",
    "                            pred, _, _ = sga.NeuralNetwork(X_Test, w1, w2, hiddenLayerSize = M, outLayerSize = 10, activation_h = h)\n",
    "                            acc = np.mean( np.argmax(pred,1) == np.argmax(y_test ,1) )\n",
    "                            scores.append(acc) # save score we got\n",
    "                            results['batch_size'].append(batchSize)\n",
    "                            results['HiddenLayers M'].append(M)\n",
    "                            results['activation_h'].append(h)\n",
    "                            results['epochs'].append(epoch)\n",
    "                            results['learning_rate'].append(l_rate)\n",
    "                            results['lamda'].append(l)\n",
    "                            results['error'].append(1-acc)\n",
    "                            results['accuracy'].append(acc)\n",
    "    return scores, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current M:  100  | Current h:  h1  | Current epoch:  10\n",
      "Current M:  100  | Current h:  h1  | Current epoch:  20\n",
      "Current M:  100  | Current h:  h2  | Current epoch:  10\n",
      "Current M:  100  | Current h:  h2  | Current epoch:  20\n",
      "Current M:  100  | Current h:  h3  | Current epoch:  10\n",
      "Current M:  100  | Current h:  h3  | Current epoch:  20\n",
      "Current M:  200  | Current h:  h1  | Current epoch:  10\n",
      "Current M:  200  | Current h:  h1  | Current epoch:  20\n",
      "Current M:  200  | Current h:  h2  | Current epoch:  10\n",
      "Current M:  200  | Current h:  h2  | Current epoch:  20\n",
      "Current M:  200  | Current h:  h3  | Current epoch:  10\n",
      "Current M:  200  | Current h:  h3  | Current epoch:  20\n",
      "Current M:  300  | Current h:  h1  | Current epoch:  10\n",
      "Current M:  300  | Current h:  h1  | Current epoch:  20\n",
      "Current M:  300  | Current h:  h2  | Current epoch:  10\n",
      "Current M:  300  | Current h:  h2  | Current epoch:  20\n",
      "Current M:  300  | Current h:  h3  | Current epoch:  10\n",
      "Current M:  300  | Current h:  h3  | Current epoch:  20\n",
      "Wall time: 2h 32min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores, data = testParameters(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "# save results\n",
    "df = pd.DataFrame(data, columns= ['batch_size', 'HiddenLayers M', 'activation_h', 'epochs', 'learning_rate', 'lamda', 'error', 'accuracy'])\n",
    "df.to_csv(\"CIFAR_RESULTS_TEST_PARAMETERS.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
